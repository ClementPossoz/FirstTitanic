{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/titanic/test.csv\n",
      "/kaggle/input/titanic/train.csv\n",
      "/kaggle/input/titanic/gender_submission.csv\n",
      "/kaggle/input/test-polynomial/__results__.html\n",
      "/kaggle/input/test-polynomial/precisions_moyennes.csv\n",
      "/kaggle/input/test-polynomial/__notebook__.ipynb\n",
      "/kaggle/input/test-polynomial/custom.css\n",
      "/kaggle/input/test-polynomial/__output__.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import sklearn as sk\n",
    "import csv\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "Data = pd.read_csv('../input/titanic/train.csv')\n",
    "test = pd.read_csv('../input/titanic/test.csv')\n",
    "PrecisionsMoyennes = pd.read_csv('../input/test-polynomial/precisions_moyennes.csv')\n",
    "\n",
    "#train.drop(columns=['Name','Ticket','Cabin'])  ---> Pourquoi ça ne marche pas ?\n",
    "#test.drop(columns=['Name','Ticket','Cabin'])\n",
    "\n",
    "#SUPRESSION DES DONNEES INUTILES\n",
    "Data.pop('Name')\n",
    "Data.pop('Ticket')\n",
    "Data.pop('Cabin')\n",
    "test.pop('Name')\n",
    "test.pop('Ticket')\n",
    "test.pop('Cabin')\n",
    "\n",
    "#Conversion des datas non-numériques\n",
    "\n",
    "Data['Sex'] = Data['Sex'].apply({'male':-1, 'female':1}.get)\n",
    "test['Sex'] = test['Sex'].apply({'male':-1, 'female':1}.get)\n",
    "Data['Embarked'] = Data['Embarked'].apply({'C':0, 'S':1, 'Q':0.5}.get)\n",
    "test['Embarked'] = test['Embarked'].apply({'C':0, 'S':1, 'Q':0.5}.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>108.9000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22.3583</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Pclass  Sex   Age  SibSp  Parch      Fare  Embarked\n",
       "0            892       3   -1  34.5      0      0    7.8292       0.5\n",
       "1            893       3    1  47.0      1      0    7.0000       1.0\n",
       "2            894       2   -1  62.0      0      0    9.6875       0.5\n",
       "3            895       3   -1  27.0      0      0    8.6625       1.0\n",
       "4            896       3    1  22.0      1      1   12.2875       1.0\n",
       "..           ...     ...  ...   ...    ...    ...       ...       ...\n",
       "413         1305       3   -1   NaN      0      0    8.0500       1.0\n",
       "414         1306       1    1  39.0      0      0  108.9000       0.0\n",
       "415         1307       3   -1  38.5      0      0    7.2500       1.0\n",
       "416         1308       3   -1   NaN      0      0    8.0500       1.0\n",
       "417         1309       3   -1   NaN      1      1   22.3583       0.0\n",
       "\n",
       "[418 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LISTE D'ALGORITHMES DE PREDICTION\n",
    "from sklearn import linear_model\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "Ridge = linear_model.RidgeClassifier()\n",
    "LinReg = linear_model.LogisticRegression()\n",
    "SDG = linear_model.SGDClassifier() #Stochastic Gradient Decent\n",
    "PassiveAgressive = linear_model.PassiveAggressiveClassifier()\n",
    "LDA = LinearDiscriminantAnalysis()\n",
    "QDA = QuadraticDiscriminantAnalysis()\n",
    "Tree = tree.DecisionTreeClassifier()\n",
    "\n",
    "Mlpc_R_L = MLPClassifier(activation = 'relu', solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(10, 2), random_state=1) # Neural Network solver = lbfgs, activation = relu\n",
    "Mlpc_R_S = MLPClassifier(activation = 'relu', solver='sgd', max_iter=1000, alpha=1e-5, hidden_layer_sizes=(10, 2), random_state=1) # Neural Network solver = sgd, activation = relu\n",
    "Mlpc_R_A = MLPClassifier(activation = 'relu', solver='adam', max_iter=1000, alpha=1e-5, hidden_layer_sizes=(10, 2), random_state=1) # Neural Network solver = adam, activation = relu\n",
    "Mlpc_T_L = MLPClassifier(activation = 'tanh', solver='lbfgs', max_iter=10000, alpha=1e-5, hidden_layer_sizes=(10, 2), random_state=1) # Neural Network solver = lbfgs, activation = tanh\n",
    "Mlpc_T_S = MLPClassifier(activation = 'tanh', solver='sgd', max_iter=1000, alpha=1e-5, hidden_layer_sizes=(10, 2), random_state=1) # Neural Network solver = sgd, activation = tanh\n",
    "Mlpc_T_A = MLPClassifier(activation = 'tanh', solver='adam', max_iter=1000, alpha=1e-5, hidden_layer_sizes=(10, 2), random_state=1) # Neural Network solver = adam, activation = tanh\n",
    "Mlpc_L_L = MLPClassifier(activation = 'logistic', solver='lbfgs', max_iter=10000, alpha=1e-5, hidden_layer_sizes=(10, 2), random_state=1) # Neural Network solver = lbfgs, activation = logistic\n",
    "Mlpc_L_S = MLPClassifier(activation = 'logistic', solver='sgd', max_iter=1000, alpha=1e-5, hidden_layer_sizes=(10, 2), random_state=1) # Neural Network solver = sgd, activation = logistic\n",
    "Mlpc_L_A = MLPClassifier(activation = 'logistic', solver='adam', max_iter=1000, alpha=1e-5, hidden_layer_sizes=(10, 2), random_state=1) # Neural Network solver = adam, activation = logistic\n",
    "\n",
    "SVC = sk.svm.SVC()\n",
    "Kneighbors = sk.neighbors.KNeighborsClassifier()\n",
    "GaussProcess = GaussianProcessClassifier()\n",
    "GaussNB = GaussianNB()\n",
    "RandomForest = RandomForestClassifier()\n",
    "AdaBoost = AdaBoostClassifier()\n",
    "\n",
    "\n",
    "nom_algo = ['Ridge', 'LinReg', 'SDG', 'PassiveAgressive', 'LDA', 'QDA', 'Tree', 'Mlpc_R_L', 'Mlpc_R_S', 'Mlpc_R_A', 'Mlpc_T_L', 'Mlpc_T_S', 'Mlpc_T_A', 'Mlpc_L_L', 'Mlpc_L_S', 'Mlpc_L_A', 'SVC', 'Kneighbors', 'GaussProcess', 'GaussNB', 'RandomForest', 'AdaBoost']\n",
    "liste_algos=[Ridge, LinReg, SDG, PassiveAgressive, LDA, QDA, Tree, Mlpc_R_L, Mlpc_R_S, Mlpc_R_A, Mlpc_T_L,Mlpc_T_S, Mlpc_T_A, Mlpc_L_L, Mlpc_L_S, Mlpc_L_A, SVC,Kneighbors, GaussProcess, GaussNB, RandomForest, AdaBoost]\n",
    "algorithmes = pd.DataFrame({'Nom':nom_algo, 'Algorithme':liste_algos})\n",
    "algorithmes.set_index('Nom', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_degres=[]\n",
    "col_nom_algo=[]\n",
    "precisions_moyennes=PrecisionsMoyennes.copy()\n",
    "for nom in nom_algo:\n",
    "    col_degres = col_degres+[1, 2, 3, 4, 5, 6]\n",
    "    col_nom_algo=col_nom_algo+[nom]*6\n",
    "precisions_moyennes.insert(0, 'Algorithme', col_nom_algo)\n",
    "precisions_moyennes.insert(1, 'Degre', col_degres)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def data_interp(data):\n",
    "# GENERATION ET NORMALISATION DES DONNEES\n",
    "    dat = data.copy()\n",
    "    y=pd.DataFrame()\n",
    "    if 'Survived' in dat.columns:\n",
    "        y = dat['Survived'].copy()\n",
    "    Id = dat['PassengerId'].copy()\n",
    "    \n",
    "\n",
    "    if 'Survived' in dat.columns:\n",
    "        X=dat.drop(columns = ['Survived','PassengerId'])\n",
    "    else: X=dat.drop(columns = ['PassengerId'])\n",
    "\n",
    "    X_col=list(X.columns)\n",
    "# NORMALISATION\n",
    "    X = (X - X.mean()) / (X.max() - X.min())\n",
    "    \n",
    "# MODIFICATION : NAN->INTERPOLATION\n",
    "    if sum(X.isnull().values.ravel()) != 0:\n",
    "        from sklearn.experimental import enable_iterative_imputer\n",
    "        from sklearn.impute import IterativeImputer\n",
    "        imp = IterativeImputer(random_state=0)\n",
    "        X = imp.fit_transform(X)\n",
    "    \n",
    "    X=pd.DataFrame(X, columns=X_col)\n",
    "    print(X)\n",
    "    \n",
    "    return [X, Id, y] if 'Survived' in dat.columns else [X, Id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_mean(data):\n",
    "# GENERATION ET NORMALISATION DES DONNEES\n",
    "    dat = data.copy()\n",
    "    if 'Survived' in dat.columns:\n",
    "        y = dat['Survived'].copy()\n",
    "    Id = dat['PassengerId'].copy()\n",
    "    if 'Survived' in dat.columns:\n",
    "        X=dat.drop(columns = ['Survived','PassengerId'])\n",
    "    else: X=dat.drop(columns = ['PassengerId'])\n",
    "    \n",
    "# MODIFICATION : NAN->MEAN\n",
    "    colonnes_a_corriger = [col for col in dat.columns if dat[col].isnull().any()]\n",
    "    for col in colonnes_a_corriger:\n",
    "        colonnes = X[col].mean()\n",
    "        X[col]=X[col].fillna(colonnes)\n",
    "    \n",
    "# NORMALISATION\n",
    "    X = (X - X.mean()) / (X.max() - X.min())\n",
    "    \n",
    "    return [X, Id, y] if 'Survived' in dat.columns else [X, Id]\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_suppr(data):\n",
    "# GENERATION ET NORMALISATION DES DONNEES\n",
    "    dat = data.copy()\n",
    "    dat.dropna(inplace=True)\n",
    "    if 'Survived' in dat.columns:\n",
    "        y = dat['Survived'].copy()\n",
    "    Id = dat['PassengerId'].copy()\n",
    "    if 'Survived' in dat.columns:\n",
    "        X=dat.drop(columns = ['Survived','PassengerId'])\n",
    "    else: X=dat.drop(columns = ['PassengerId'])\n",
    "    \n",
    "# NORMALISATION\n",
    "    X = (X - X.mean()) / (X.max() - X.min())\n",
    "    \n",
    "    return [X, Id, y] if 'Survived' in dat.columns else [X, Id]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CROSS-VALIDATION SUR LE DEGRE\n",
    "def precision_poly(X_train_initial, y_train, X_cross_initial, y_cross, col_poly, algo, degre_max=10, nb_iter=10):\n",
    "    # ENTREES : X_train_initial et y_train : données et résultats du train set | X_cross_initial et y_cross: données et résultats du set de cross-validation | \n",
    "    #           col_fixes : colonnes des données qu'on veut à des degrés supérieurs | degre_max : exposant maximum du polynome des données |\n",
    "    #           nb_iter : nombre d'itérations pour le calcul moyen de la précision\n",
    "    #\n",
    "    #SORTIE : DataFrame contenant les colonnes : Degré (degré du polynome des données) (index)\n",
    "    #                                            Precision_train : précision moyenne sur le set de training\n",
    "    #                                            Precision_cross : précision moyenne sur le set de cross-vaisation\n",
    "    \n",
    "    train_prec = []\n",
    "    cross_prec = []\n",
    "    \n",
    "    for d in range(1,degre_max+1):\n",
    "        X_train = Polynomiation(X_train_initial, col_poly, d)\n",
    "        X_cross = Polynomiation(X_cross_initial, col_poly, d)       \n",
    "        [p_train, p_cross] = PrecisionMoyenne_train_cross(X_train, y_train, X_cross, y_cross, algo, nb_iter)\n",
    "        \n",
    "        train_prec.append(p_train)\n",
    "        cross_prec.append(p_cross)\n",
    "\n",
    "    prec_degre = pd.DataFrame({'Degre':list(range(1,degre_max+1)), 'Précision_Train':train_prec, 'Précision_Cross':cross_prec})\n",
    "    prec_degre.set_index('Degre', inplace=True) \n",
    "    \n",
    "    return prec_degre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrecisionMoyenne_train_cross(X_train_entree, y_train_entree, X_cross_entree, y_cross_entree, algo, nb_iter=10): \n",
    "#Renvoie la précision moyenne sur nb_iter itérations de l'algorithme sur les données de de train et de cross-validation \n",
    "#(si l'algorithme est déterministe, la précision est calculée seulement sur 1 iteration)  \n",
    "    \n",
    "    algorithme=algorithmes.loc[algo, 'Algorithme']\n",
    "    \n",
    "    X_train=X_train_entree.copy()\n",
    "    y_train=y_train_entree.copy()\n",
    "    X_cross=X_cross_entree.copy()\n",
    "    y_cros=y_cross_entree.copy()\n",
    "\n",
    "    prec_train = []\n",
    "    prec_cross = []\n",
    "    \n",
    "\n",
    "    for it in range(nb_iter):\n",
    "        algorithme.fit(X_train, y_train)\n",
    "        prediction_train = np.array(algorithme.predict(X_train))\n",
    "        precision_train = (prediction_train==y_train).mean()\n",
    "        prec_train.append(precision_train)\n",
    "        \n",
    "        prediction_cross = np.array(algorithme.predict(X_cross))\n",
    "        precision_cross = (prediction_cross==y_cross).mean()\n",
    "        prec_cross.append(precision_cross)\n",
    "        if prec_cross[it]==prec_cross[it-1]: break\n",
    "    \n",
    "    precision_moyenne_train = np.array(prec_train).mean()\n",
    "    precision_moyenne_cross = np.array(prec_cross).mean()\n",
    "    \n",
    "    return [precision_moyenne_train, precision_moyenne_cross]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARATION DES DONNEES POUR CROSS-VALIDATION ET TEST\n",
    "# Entrée : Données à séparer en groupe train / cross / test & Méthode de traitement des données manquante\n",
    "#Sortie : Dataframe contenant les données, l'identifiant et le résultat attendu pour chaque groupe Train, Cross et TestCross\n",
    "\n",
    "def preparation(data, methode):\n",
    "    ratio_TrainReste = 0.8\n",
    "    ratio_CrossTest = 1\n",
    "\n",
    "    [train, reste]=Split(data, ratio_TrainReste)\n",
    "    [cross, testcross]=Split(reste, ratio_CrossTest)\n",
    "    Train = [X_train_total, Id_train, y_train] = methode(train)\n",
    "    Cross = [X_cross_total, Id_cross, y_cross] = methode(cross)\n",
    "    TestCross= [X_testcross, Id_testcross, y_testcross] = methode(testcross)\n",
    "    \n",
    "    dat = pd.DataFrame({'Donnee':['X', 'Id', 'y'], 'Train':Train, 'Cross':Cross, 'TestCross':TestCross})\n",
    "    dat.set_index('Donnee', inplace=True)\n",
    "    return dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    " def Polynomiation(donnee, col_polynomes, deg):\n",
    "    col_fixes=set(donnee.columns.values.tolist())-set(col_polynomes)\n",
    "    donneePoly = donnee[col_fixes].copy()\n",
    "    donneePoly['Sex*Age']=donnee['Sex'].mul(donnee['Age'])\n",
    "    donneePoly['Sex*Embarked']=donnee['Sex'].mul(donnee['Embarked'])\n",
    "    donneePoly['Sex*Pclass']=donnee['Sex'].mul(donnee['Pclass'])\n",
    "\n",
    "    for degre in range(1,deg+1):\n",
    "        for col in col_polynomes :\n",
    "            donneePoly[col+'^'+str(degre)]=donnee[col].pow(degre)\n",
    "\n",
    "    return donneePoly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ----- SEPARATION POUR CROSS-VALIDATION -----\n",
    "def Split(donnee, ratio):\n",
    "\n",
    "    #SHUFFLE\n",
    "    donneesplit=donnee.sample(frac=1).copy()\n",
    "\n",
    "    # SEPARATION\n",
    "    part1 = donneesplit[:int(len(Data)*ratio)]\n",
    "    part2 = donneesplit[int(len(Data)*ratio):]\n",
    "    return [part1, part2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" # ------------ CROSS-VALIDATION : MOYENNE DES ALGORITHMES ------------ \\n\\nMethode = data_interp\\nnoms_algos = algorithmes.index\\ndegre_max=6\\nnb_iter=10\\n\\ndonnees_preparees = preparation(Data, Methode)\\n[X_train_initial, Id_train, y_train] = donnees_preparees['Train']\\n[X_cross_initial, Id_cross, y_cross] = donnees_preparees['Cross']\\n[X_testcross, Id_testcross, y_testcross] = donnees_preparees['TestCross']\\n\\ncol_poly=['Pclass', 'Age','SibSp', 'Parch', 'Fare']\\n\\nresults = {}\\n\\nfor d in range(1,degre_max+1):\\n    for i in range(len(nom_algo)):\\n        algo = noms_algos[i]\\n        prec = precision_poly(X_train_initial, y_train, X_cross_initial, y_cross, col_poly, algo, degre_max, nb_iter)\\n        results[algo]=prec\\n            \\nprecisions = pd.concat(results)  \""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' # ------------ CROSS-VALIDATION : MOYENNE DES ALGORITHMES ------------ \n",
    "\n",
    "Methode = data_interp\n",
    "noms_algos = algorithmes.index\n",
    "degre_max=6\n",
    "nb_iter=10\n",
    "\n",
    "donnees_preparees = preparation(Data, Methode)\n",
    "[X_train_initial, Id_train, y_train] = donnees_preparees['Train']\n",
    "[X_cross_initial, Id_cross, y_cross] = donnees_preparees['Cross']\n",
    "[X_testcross, Id_testcross, y_testcross] = donnees_preparees['TestCross']\n",
    "\n",
    "col_poly=['Pclass', 'Age','SibSp', 'Parch', 'Fare']\n",
    "\n",
    "results = {}\n",
    "\n",
    "for d in range(1,degre_max+1):\n",
    "    for i in range(len(nom_algo)):\n",
    "        algo = noms_algos[i]\n",
    "        prec = precision_poly(X_train_initial, y_train, X_cross_initial, y_cross, col_poly, algo, degre_max, nb_iter)\n",
    "        results[algo]=prec\n",
    "            \n",
    "precisions = pd.concat(results)  '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# PREDICTION MOYENNE \\n\\nMethode = data_interp\\nnoms_algos = algorithmes.index\\ndegre_max = 6\\n\\ndonnees_preparees = preparation(Data, Methode)\\n[X_train_initial, Id_train, y_train] = donnees_interp(['Train'])\\n[X_cross_initial, Id_cross, y_cross] = donnees_preparees['Cross']\\n\\ncol_poly=['Pclass', 'Age','SibSp', 'Parch', 'Fare']\\n\\npredictions = []\\n\\nfor i in range(len(nom_algo)):\\n    for d in range(1,degre_max+1):\\n        algo = noms_algos[i]\\n        algorithme=algorithmes.loc[algo, 'Algorithme']\\n        \\n        X_train = Polynomiation(X_train_initial, col_poly, d)\\n        X_cross = Polynomiation(X_cross_initial, col_poly, d) \\n        \\n        algorithme.fit(X_train, y_train)\\n        pred_degre_algo = np.array(algorithme.predict(X_cross))\\n        predictions.append(pred_degre_algo)\\n        \\n\\n        \\npredictions=np.array(predictions)\\npredictions[predictions==0]=-1\\n\\nprediction_moyenne = np.array(precisions_moyennes['Précision_Cross']).dot(predictions)\\nprediction_moyenne[prediction_moyenne>0]=1\\nprediction_moyenne[prediction_moyenne<0]=0\\n\\n\\nprecision_finale = (prediction_moyenne==y_cross).mean()\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# PREDICTION MOYENNE \n",
    "\n",
    "Methode = data_interp\n",
    "noms_algos = algorithmes.index\n",
    "degre_max = 6\n",
    "\n",
    "donnees_preparees = preparation(Data, Methode)\n",
    "[X_train_initial, Id_train, y_train] = donnees_interp(['Train'])\n",
    "[X_cross_initial, Id_cross, y_cross] = donnees_preparees['Cross']\n",
    "\n",
    "col_poly=['Pclass', 'Age','SibSp', 'Parch', 'Fare']\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i in range(len(nom_algo)):\n",
    "    for d in range(1,degre_max+1):\n",
    "        algo = noms_algos[i]\n",
    "        algorithme=algorithmes.loc[algo, 'Algorithme']\n",
    "        \n",
    "        X_train = Polynomiation(X_train_initial, col_poly, d)\n",
    "        X_cross = Polynomiation(X_cross_initial, col_poly, d) \n",
    "        \n",
    "        algorithme.fit(X_train, y_train)\n",
    "        pred_degre_algo = np.array(algorithme.predict(X_cross))\n",
    "        predictions.append(pred_degre_algo)\n",
    "        \n",
    "\n",
    "        \n",
    "predictions=np.array(predictions)\n",
    "predictions[predictions==0]=-1\n",
    "\n",
    "prediction_moyenne = np.array(precisions_moyennes['Précision_Cross']).dot(predictions)\n",
    "prediction_moyenne[prediction_moyenne>0]=1\n",
    "prediction_moyenne[prediction_moyenne<0]=0\n",
    "\n",
    "\n",
    "precision_finale = (prediction_moyenne==y_cross).mean()'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Pclass       Sex       Age     SibSp     Parch      Fare  Embarked\n",
      "0    0.345679 -0.352413 -0.096747  0.059624 -0.063599 -0.048707  0.232283\n",
      "1   -0.654321  0.647587  0.104309  0.059624 -0.063599  0.076277 -0.767717\n",
      "2    0.345679  0.647587 -0.046483 -0.065376 -0.063599 -0.047390  0.232283\n",
      "3   -0.654321  0.647587  0.066611  0.059624 -0.063599  0.040786  0.232283\n",
      "4    0.345679 -0.352413  0.066611 -0.065376 -0.063599 -0.047146  0.232283\n",
      "..        ...       ...       ...       ...       ...       ...       ...\n",
      "886 -0.154321 -0.352413 -0.033917 -0.065376 -0.063599 -0.037484  0.232283\n",
      "887 -0.654321  0.647587 -0.134445 -0.065376 -0.063599 -0.004302  0.232283\n",
      "888  0.345679  0.647587 -0.125699  0.059624  0.269734 -0.017087  0.232283\n",
      "889 -0.654321 -0.352413 -0.046483 -0.065376 -0.063599 -0.004302 -0.767717\n",
      "890  0.345679 -0.352413  0.028913 -0.065376 -0.063599 -0.047731 -0.267717\n",
      "\n",
      "[891 rows x 7 columns]\n",
      "       Pclass       Sex       Age     SibSp     Parch      Fare  Embarked\n",
      "0    0.367225 -0.363636  0.055749 -0.055921 -0.043594 -0.054258 -0.200957\n",
      "1    0.367225  0.636364  0.220591  0.069079 -0.043594 -0.055877  0.299043\n",
      "2   -0.132775 -0.363636  0.418402 -0.055921 -0.043594 -0.050631 -0.200957\n",
      "3    0.367225 -0.363636 -0.043157 -0.055921 -0.043594 -0.052632  0.299043\n",
      "4    0.367225  0.636364 -0.109094  0.069079  0.067517 -0.045556  0.299043\n",
      "..        ...       ...       ...       ...       ...       ...       ...\n",
      "413  0.367225 -0.363636 -0.074393 -0.055921 -0.043594 -0.053827  0.299043\n",
      "414 -0.632775  0.636364  0.115092 -0.055921 -0.043594  0.143019 -0.700957\n",
      "415  0.367225 -0.363636  0.108498 -0.055921 -0.043594 -0.055389  0.299043\n",
      "416  0.367225 -0.363636 -0.074393 -0.055921 -0.043594 -0.053827  0.299043\n",
      "417  0.367225 -0.363636 -0.086757  0.069079  0.067517 -0.025899 -0.700957\n",
      "\n",
      "[418 rows x 7 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:691: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:691: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:691: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:691: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "# PREDICTION MOYENNE : ECHANTILLON TEST - pour soumission \n",
    "\n",
    "Methode = data_interp\n",
    "noms_algos = algorithmes.index\n",
    "degre_max = 6\n",
    "\n",
    "[X_train_initial, Id_train, y_train] = data_interp(Data)\n",
    "[X_test_initial, Id_test]=data_interp(test)\n",
    "\n",
    "col_poly=['Pclass', 'Age','SibSp', 'Parch', 'Fare']\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i in range(len(nom_algo)):\n",
    "    for d in range(1,degre_max+1):\n",
    "        algo = noms_algos[i]\n",
    "        algorithme=algorithmes.loc[algo, 'Algorithme']\n",
    "        \n",
    "        X_train = Polynomiation(X_train_initial, col_poly, d)\n",
    "        X_test = Polynomiation(X_test_initial, col_poly, d) \n",
    "        \n",
    "        algorithme.fit(X_train, y_train)\n",
    "        pred_degre_algo = np.array(algorithme.predict(X_test))\n",
    "        predictions.append(pred_degre_algo)\n",
    "        \n",
    "\n",
    "        \n",
    "predictions=np.array(predictions)\n",
    "predictions[predictions==0]=-1\n",
    "\n",
    "prediction_test = np.array(precisions_moyennes['Précision_Cross']).dot(predictions)\n",
    "\n",
    "prediction_test[prediction_test<0]=int(0)\n",
    "prediction_test[prediction_test>0]=int(1)\n",
    "\n",
    "sortie = pd.DataFrame({'PassengerId' : Id_test, 'Survived' : prediction_test}).astype(int)\n",
    "sortie.to_csv('prediction_moyenne.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with pd.option_context('display.max_rows', None, 'display.max_columns', None): print(pd.DataFrame(predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
