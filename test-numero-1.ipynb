{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport sklearn as sk\nimport csv\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"Data = pd.read_csv('../input/titanic/train.csv')\ntest = pd.read_csv('../input/titanic/test.csv')\n#train.drop(columns=['Name','Ticket','Cabin'])  ---> Pourquoi ça ne marche pas ?\n#test.drop(columns=['Name','Ticket','Cabin'])\nData.pop('Name')\nData.pop('Ticket')\nData.pop('Cabin')\ntest.pop('Name')\ntest.pop('Ticket')\ntest.pop('Cabin')\n\ntrain = Data[:int(len(Data)*0.7)].copy()\ncross = Data[int(len(Data)*0.7):].copy()\n\n#combine = [train, test]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RECHERCHE DES COLONNES INCOMPLETES\ntest_incompletes=[col for col in test.columns if test[col].isnull().any()]\ntrain_incompletes=[col for col in train.columns if train[col].isnull().any()]\ncross_incompletes=[col for col in cross.columns if cross[col].isnull().any()]\n\ntrain_incompletes, test_incompletes, cross_incompletes\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Conversion des datas non-numériques\n\ntrain['Sex'] = train['Sex'].apply({'male':0, 'female':1}.get)\ncross['Sex'] = cross['Sex'].apply({'male':0, 'female':1}.get)\ntest['Sex'] = test['Sex'].apply({'male':0, 'female':1}.get)\ntrain['Embarked'] = train['Embarked'].apply({'C':0, 'S':1, 'Q':0.5}.get)\ncross['Embarked'] = cross['Embarked'].apply({'C':0, 'S':1, 'Q':0.5}.get)\ntest['Embarked'] = test['Embarked'].apply({'C':0, 'S':1, 'Q':0.5}.get)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ETUDE DE L'INFLUENCE DU LIEU D'EMBARQUEMENT\n\n#Passagers embarqués à S\nS=train.loc[(train['Embarked']==1),:]\n#dont survivants\nSV=S.loc[(S['Survived']==1),:]\n# taux de survie\ntaux_Embarked_S=SV.Embarked.count()/S.Embarked.count()\n\n#Passagers embarqués à C\nC=train.loc[(train['Embarked']==0),:]\n#dont survivants\nCV=C.loc[(C['Survived']==1),:]\n# taux de survie\ntaux_Embarked_C=CV.Embarked.count()/C.Embarked.count()\n\nprint('taux de survie des passagers de S =',taux_Embarked_S,' ; taux de survie des passagers de S =',taux_Embarked_C)\n\n\n\n\n\n\n# Proportion des passagers dans chaque classe, selon leur ville d'embarquement\n\n#Passagers embarqués à S\n#S=train.loc[(train['Embarked']==\"S\"),['Survived','Embarked']]\n#dont 1ère classe\nS1=S.loc[(S['Pclass']==1),:]\n#dont 2ème classe\nS2=S.loc[(S['Pclass']==2),:]\n#dont 3ème classe\nS3=S.loc[(S['Pclass']==3),:]\n# proportion des passagers de S dans chaque classe :\nprop_S_1=S1.Embarked.count()/S.Embarked.count()\nprop_S_2=S2.Embarked.count()/S.Embarked.count()\nprop_S_3=S3.Embarked.count()/S.Embarked.count()\n\n#Passagers embarqués à C\n#S=train.loc[(train['Embarked']==\"C\"),['Survived','Embarked']]\n#dont 1ère classe\nC1=C.loc[(C['Pclass']==1),:]\n#dont 2ème classe\nC2=C.loc[(C['Pclass']==2),:]\n#dont 3ème classe\nC3=C.loc[(C['Pclass']==3),:]\n# proportion des passagers de S dans chaque classe :\nprop_C_1=C1.Embarked.count()/C.Embarked.count()\nprop_C_2=C2.Embarked.count()/C.Embarked.count()\nprop_C_3=C3.Embarked.count()/C.Embarked.count()\n\nprint('proportion de passagers de S en 1ère =',prop_S_1,' ; proportion de passagers de S en 2ème =',prop_S_2,' ; proportion de passagers de S en 3ème =', prop_S_3)\nprint('proportion de passagers de C en 1ère =',prop_C_1,' ; proportion de passagers de C en 2ème =',prop_C_2,' ; proportion de passagers de C en 3ème =', prop_C_3)\n\n\n\n\n# A classe égale, il y a t-il de différence de taux de survie selon le lieu d'embarquement ?\n\nS1S=S1.loc[(S['Survived']==1),:] #Embarqué à S, classe 1, Survivant\nS2S=S2.loc[(S['Survived']==1),:] #Embarqué à S, classe 2, Survivant\nS3S=S3.loc[(S['Survived']==1),:] #Embarqué à S, classe 3, Survivant\nC1S=C1.loc[(C['Survived']==1),:] #Embarqué à C, classe 1, Survivant\nC2S=C2.loc[(C['Survived']==1),:] #Embarqué à C, classe 2, Survivant\nC3S=C3.loc[(C['Survived']==1),:] #Embarqué à C, classe 3, Survivant\n\ntaux_S1=S1S.Embarked.count()/S1.Embarked.count()\ntaux_S2=S2S.Embarked.count()/S2.Embarked.count()\ntaux_S3=S3S.Embarked.count()/S3.Embarked.count()\ntaux_C1=C1S.Embarked.count()/C1.Embarked.count()\ntaux_C2=C2S.Embarked.count()/C2.Embarked.count()\ntaux_C3=C3S.Embarked.count()/C3.Embarked.count()\n\nprint('taux de survie des passagers de S en 1ère =',taux_S1,' ; taux de survie des passagers de S en 2ème =',taux_S2,' ; taux de survie des passagers de S en 3ème =', taux_S3)\nprint('taux de survie des passagers de C en 1ère =',taux_C1,' ; taux de survie des passagers de C en 2ème =',taux_C2,' ; taux de survie des passagers de C en 3ème =', taux_C3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CROSSTABS & PLOTS\n\nprint(pd.crosstab(train['Survived'],train['SibSp']))\nprint(pd.crosstab(train['Survived'],train['Parch']))\nprint(pd.crosstab(train['Survived'],train['Embarked']))\nprint(pd.crosstab(train['Pclass'],train['Embarked']))\n\n#Plots\nsurv=train['Survived']\nsurv = surv.apply({0:'g', 1:'b'}.get)\n\ntrain.plot('Pclass','Age', c=surv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def donnees_suppr() :\n\n# DONNEES SANS LES DONNEES INCOMPLETES\n    train_suppr = train.copy()\n    train_suppr.dropna(inplace=True)\n\n# NORMALISATION DES DONNEES\n    y_suppr=train_suppr['Survived'].copy()\n    Id_X = train_suppr['PassengerId'].copy()\n    X_suppr=train_suppr.drop(columns = ['Survived', 'PassengerId'])\n\n    X_suppr = (X_suppr - X_suppr.mean()) / (X_suppr.max() - X_suppr.min())\n\n# PREPARATION DES DONNEES TEST\n    test_suppr = test.copy()\n    test_suppr.dropna(inplace=True)\n    \n    Id_test = test_suppr['PassengerId'].copy()\n    test_suppr=test_suppr.drop(columns = 'PassengerId')\n\n# NORMALISATION DES DONNEES TEST\n    test_suppr = (test_suppr - test_suppr.mean()) / (test_suppr.max() - test_suppr.min())\n\n\n    return X_suppr , test_suppr, y_suppr, Id_test\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##TEST DONNES_SUPPR\n\n# DONNEES SANS LES DONNEES INCOMPLETES\ntrain_suppr = train.copy()\ntrain_suppr.dropna(inplace=True)\n\n# NORMALISATION DES DONNEES\n\ny_suppr=train_suppr['Survived'].copy()\nId_X = train_suppr['PassengerId'].copy()\nX_suppr=train_suppr.drop(columns = ['Survived', 'PassengerId'])\n\nX_suppr = (X_suppr - X_suppr.mean()) / (X_suppr.max() - X_suppr.min())\n\n# PREPARATION DES DONNEES TEST\ntest_suppr = test.copy()\ntest_suppr.dropna(inplace=True)\n    \nId_test = test_suppr['PassengerId'].copy()\ntest_suppr=test_suppr.drop(columns = 'PassengerId')\n\n# NORMALISATION DES DONNEES TEST\ntest_suppr = (test_suppr - test_suppr.mean()) / (test_suppr.max() - test_suppr.min())\n\nX_suppr , test_suppr, y_suppr, Id_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#VERIFICATION : NOMBRE DE DONNEES MANQUANTES\ntrainmanquantes=donnees_suppr()[0]\ntestmanquantes=donnees_suppr()[1]\nsum(trainmanquantes.isnull().values.ravel()),sum(testmanquantes.isnull().values.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#DONNEES INCOMPLETES -> MEAN\n\ndef donnees_mean():\n# GENERATION ET NORMALISATION DES DONNEES\n    train_mean = train.copy()\n    y_mean=train_mean['Survived'].copy()\n    Id_X = train_mean['PassengerId'].copy()\n    X_mean=train_mean.drop(columns = ['Survived', 'PassengerId'])\n\n    X_mean = (X_mean - X_mean.mean()) / (X_mean.max() - X_mean.min())\n\n# MODIFICATION : NAN->MEAN\n    colonnes_a_corriger = train_incompletes\n    for col in colonnes_a_corriger:\n        col_mean = X_mean[col].mean()\n        X_mean[col]=X_mean[col].fillna(col_mean)\n\n\n\n#PREPARATION DES DONNEES TEST\n    test_mean = test.copy()\n    \n    Id_test = test_mean['PassengerId'].copy()\n    test_mean=test_mean.drop(columns = 'PassengerId')\n    \n    test_mean=(test_mean - test_mean.mean()) / (test_mean.max() - test_mean.min())\n\n# MODIFICATION DES DONNEES TEST : NAN->MEAN\n    colonnes_a_corriger = test_incompletes\n    for col in colonnes_a_corriger:\n        col_mean = test_mean[col].mean()\n        test_mean[col]=test_mean[col].fillna(col_mean)\n\n\n    return X_mean, test_mean, y_mean, Id_test\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#VERIFICATION : NOMBRE DE DONNEES MANQUANTES\ntrainmanquantes=donnees_mean()[0]\ntestmanquantes=donnees_mean()[1]\nsum(trainmanquantes.isnull().values.ravel()),sum(testmanquantes.isnull().values.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#DONNEES INCOMPLETES -> INTREPOLATION\n\ndef donnees_interpolees():\n# GENERATION ET NORMALISATION DES DONNEES\n    train_interp = train.copy()\n    y_interp=train_interp['Survived'].copy()\n    Id_X = train_interp['PassengerId'].copy()\n    X_interp=train_interp.drop(columns = ['Survived', 'PassengerId'])\n\n    X_interp = (X_interp - X_interp.mean()) / (X_interp.max() - X_interp.min())\n    \n#PREPARATION DES DONNEES TEST\n    test_interp = test.copy()\n    \n    Id_test = test_interp['PassengerId'].copy()\n    test_interp=test_interp.drop(columns = 'PassengerId')\n    \n    test_interp = (test_interp - test_interp.mean()) / (test_interp.max() - test_interp.min())\n\n# MODIFICATION : NAN->INTERPOLATION\n    from sklearn.experimental import enable_iterative_imputer\n    from sklearn.impute import IterativeImputer\n    imp = IterativeImputer(random_state=0)\n    X_interp = imp.fit_transform(X_interp)\n    test_interp = imp.fit_transform(test_interp)\n\n    return X_interp, test_interp, y_interp, Id_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#VERIFICATION : NOMBRE DE DONNEES MANQUANTES\ntrainmanquantes=donnees_interpolees()[0]\ntestmanquantes=donnees_interpolees()[1]\nnp.count_nonzero(np.isnan(trainmanquantes)), np.count_nonzero(np.isnan(testmanquantes)), ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LISTE D'ALGORITHMES DE PREDICTION\nfrom sklearn import linear_model\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nRidge = linear_model.RidgeClassifier()\nLinReg = linear_model.LogisticRegression()\nSDG = linear_model.SGDClassifier() #Stochastic Gradient Decent\nPassiveAgressive = linear_model.PassiveAggressiveClassifier()\nLDA = LinearDiscriminantAnalysis()\nQDA = QuadraticDiscriminantAnalysis()\nTree = tree.DecisionTreeClassifier()\n\nMlpc_R_L = MLPClassifier(activation = 'relu', solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(10, 2), random_state=1) # Neural Network solver = lbfgs, activation = relu\nMlpc_R_S = MLPClassifier(activation = 'relu', solver='sgd', max_iter=1000, alpha=1e-5, hidden_layer_sizes=(10, 2), random_state=1) # Neural Network solver = lbfgs, activation = sgd\nMlpc_R_A = MLPClassifier(activation = 'relu', solver='adam', max_iter=1000, alpha=1e-5, hidden_layer_sizes=(10, 2), random_state=1) # Neural Network solver = lbfgs, activation = adam\nMlpc_T_L = MLPClassifier(activation = 'tanh', solver='lbfgs', max_iter=1000, alpha=1e-5, hidden_layer_sizes=(10, 2), random_state=1) # Neural Network solver = tanh, activation = relu\nMlpc_T_S = MLPClassifier(activation = 'tanh', solver='sgd', max_iter=1000, alpha=1e-5, hidden_layer_sizes=(10, 2), random_state=1) # Neural Network solver = tanh, activation = sgd\nMlpc_T_A = MLPClassifier(activation = 'tanh', solver='adam', max_iter=1000, alpha=1e-5, hidden_layer_sizes=(10, 2), random_state=1) # Neural Network solver = tanh, activation = adam\nMlpc_L_L = MLPClassifier(activation = 'logistic', solver='lbfgs', max_iter=1000, alpha=1e-5, hidden_layer_sizes=(10, 2), random_state=1) # Neural Network solver = logistic, activation = relu\nMlpc_L_S = MLPClassifier(activation = 'logistic', solver='sgd', max_iter=1000, alpha=1e-5, hidden_layer_sizes=(10, 2), random_state=1) # Neural Network solver = logistic, activation = sgd\nMlpc_L_A = MLPClassifier(activation = 'logistic', solver='adam', max_iter=1000, alpha=1e-5, hidden_layer_sizes=(10, 2), random_state=1) # Neural Network solver = logistic, activation = adam\n\nSVC = sk.svm.SVC()\nKneighbors = sk.neighbors.KNeighborsClassifier()\nGaussProcess = GaussianProcessClassifier()\nGaussNB = GaussianNB()\nRandomForest = RandomForestClassifier()\nAdaBoost = AdaBoostClassifier()\n\nalgorithme=[Ridge, LinReg, SDG, PassiveAgressive, LDA, QDA, Tree, Mlpc_R_L, Mlpc_R_S, Mlpc_R_A, Mlpc_T_L,Mlpc_T_S, Mlpc_T_A, Mlpc_L_L, Mlpc_L_S, Mlpc_L_A, SVC,Kneighbors, GaussProcess, RandomForest, AdaBoost]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TEST SVM - SVC\ntest_SVM_SVC = sk.svm.SVC()\ntest_SVM_SVC.fit(X_suppr, y_suppr)\ntest_SVM_SVC.predict(test_suppr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"methode = [donnees_suppr(), donnees_mean(), donnees_interpolees()]\nnom_methode = ['suppr','mean','interpolation']\nnom_algo = ['Ridge', 'LinReg', 'SDG', 'PassiveAgressive', 'LDA', 'QDA', 'Tree', 'Mlpc_R_L', 'Mlpc_R_S', 'Mlpc_R_A', 'Mlpc_T_L', 'Mlpc_T_S', 'Mlpc_T_A', 'Mlpc_L_L', 'Mlpc_L_S', 'Mlpc_L_A', 'SVC', 'Kneighbors', 'GaussProcess', 'GaussNB', 'RandomForest', 'AdaBoost']\nSorties=[]\n\nfor k in range(len(methode)):\n    [data_set, data_test, y, Id]=methode[k]\n    \n    for i in range(len(algorithme)):\n        algo=algorithme[i]\n        algo.fit(data_set, y)\n        prediction = algo.predict(data_test)\n        #Sorties.append([nom_methode[k] , nom_algo[i] , prediction])\n        \n        name  = nom_methode[k]+'_'+nom_algo[i]+'.csv'\n        output = {'PassengerId' : Id, 'Survived' : prediction}\n        pd.DataFrame(output).to_csv(name, index=False)\n\n\n        \n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Sorties[0][0]+'_'+Sorties[1][0]+'.csv'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prevision = pd.read_csv('../kaggle/working/mean_GaussNB.csv')\nprevision","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"raw","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}